#%% 

import numpy as np
from pandas import read_csv
import matplotlib.pyplot as plt
from model import Net
import math
import pickle
from sklearn.preprocessing import MinMaxScaler
import warnings 
from sklearn.metrics import f1_score

#%%

def save(array, file_name):
    file = open(file_name, 'wb')
    pickle.dump(array, file)
    file.close()

def read(filename):
    with open(filename, 'rb') as file:
        array = pickle.load(file)
    return array

def ReLU(x):
    if x > 0:
        return x
    return 0.0

def sigma(x):
    if x > 0:
        return 1 / (1 + math.e ** ((-1) * x))
    return math.e ** x / (1 + math.e ** x)

def predict_class(predictions):
    classes = []
    for p in predictions:
        classes.append(np.where(np.max(p) == p))
    return np.array(classes).reshape(-1)

def predict(net, x_data):
    predictions = []
    for x in x_data:
        predictions.append(net.predict(x))
    return np.array(predictions)

#%%

df_train = read_csv("data/classification/xor3-training.csv")
df_train.head()

xy_train = df_train[["x", "y"]].to_numpy()
c_train = df_train["c"].to_numpy()

df_test = read_csv("data/classification/xor3-test.csv")
df_test.head()

xy_test = df_test[["x", "y"]].to_numpy()
c_test= df_test["c"].to_numpy()

#%%
        
plt.scatter(xy_train[:,0], xy_train[:,1], c=(c_train+0.5)/2)
plt.show()

#%% scaling

scaler_xy = MinMaxScaler(feature_range=(-0.9, 0.9))
xy_train_scaled = scaler_xy.fit_transform(xy_train)
xy_test_scaled = scaler_xy.transform(xy_test)

#%%

f = [ReLU, ReLU, "softmax"]
net_softmax = Net(n_neurons=[20, 20, 2], n_inputs=2, functions=f, param_init='xavier', use_softmax=True)
f = [ReLU, ReLU, lambda x: x]
net_basic = Net(n_neurons=[20, 20, 2], n_inputs=2, functions=f, param_init='xavier', classification=True)

#%%

warnings.filterwarnings('ignore') 

epoch = 1
score_softmax = 0
score_basic = 0
scores_softmax = []
scores_basic = []
epochs = []
while score_softmax < 0.97:
    epochs.append(epoch)
    epoch += 1
    net_softmax.fit(xy_train_scaled, c_train, batch_size=1, epochs=1, alpha=0.001,
                      method='rmsprop', m_lambda=0.9)
    net_basic.fit(xy_train_scaled, c_train, batch_size=1, epochs=1, alpha=0.001,
                      method='rmsprop', m_lambda=0.9)
    preds = predict(net_softmax, xy_test_scaled)
    classes = predict_class(preds)
    score_softmax = f1_score(c_test, classes)
    scores_softmax.append(score_softmax)
    preds = predict(net_basic, xy_test_scaled)
    classes = predict_class(preds)
    score_basic = f1_score(c_test, classes)
    scores_basic.append(score_basic)
    print(epoch - 1)
    print('s: ', score_softmax)
    print('b: ', score_basic)
    print()

#%%

plt.plot(epochs, scores_softmax, '-', markersize=1)
plt.plot(epochs, scores_basic, '-', markersize=1)
plt.legend(('softmax', 'basic'), loc='upper left')
plt.xlabel('epoch')
plt.ylabel('F1 score')
plt.show()

print('Current epoch: ', epoch - 1)
print('F1 score softmax: ', score_softmax)
print('F1 score basic: ', score_basic)

#%%


print(net_softmax.get_all_weights())
# [array([[ 1.10693599,  0.16196985],
#        [ 0.89910655,  0.03097393],
#        [-0.02359432,  0.95099664],
#        [ 1.33753307, -1.11817148],
#        [ 2.41884372,  2.54462528],
#        [-1.3476225 ,  0.06154247],
#        [-0.83177255, -0.71848551],
#        [ 0.88457102,  0.33672552],
#        [-0.32892669,  0.5323549 ],
#        [-1.91820894,  2.37119982],
#        [ 1.03498479,  0.08464917],
#        [ 1.8231517 , -1.2160684 ],
#        [ 0.68745632,  0.11191095],
#        [-0.87144335, -0.45478015],
#        [-1.75054809,  1.69260716],
#        [ 1.61073038,  1.25449322],
#        [ 0.38780091, -0.58042258],
#        [ 0.08053573, -0.10750395],
#        [-0.17103764, -0.24129762],
#        [-1.72301957,  0.3684511 ]]), array([[-8.55625582e-01, -1.65998374e+00,  7.08860494e-01,
#          4.72013933e-01,  8.77087770e-01,  3.71636956e-01,
#         -1.77665489e+00, -8.38837534e-01,  6.65289933e-01,
#          9.73478938e-01, -1.16760837e+00, -1.16750964e-01,
#          1.33545684e-01,  9.27274753e-01,  1.25051351e+00,
#          1.34128576e+00,  1.59243766e+00,  2.00018760e+00,
#          1.73225663e-01,  1.44225824e+00],
#        [-1.56738405e-01, -4.48735192e-02,  8.56345620e-02,
#          1.13574357e-03,  3.69947790e-01,  2.04779196e-01,
#         -2.17912023e-01, -2.09075397e-01, -1.45179748e-01,
#          3.71143617e-02, -3.29761668e-01, -3.85887335e-01,
#         -3.49711583e-01, -2.04892299e-01, -1.42818827e-01,
#          2.88809762e-02, -3.05959697e-01, -3.72505098e-01,
#         -2.52862555e-01, -3.77479231e-01],
#        [-6.79572132e-01,  4.77614739e-01, -4.72934045e-01,
#          5.44476125e-02,  7.62395936e-01, -9.56588413e+00,
#         -4.19349875e-01,  8.58109944e-01, -2.34967122e-01,
#         -1.16231879e+00,  2.97655034e-02, -3.69864992e-01,
#         -1.70842447e+00, -3.12603907e+00,  2.66615476e+00,
#          1.82748788e+00,  2.97784969e-01, -1.91951614e-01,
#         -2.76327563e+00, -1.40178780e+00],
#        [ 2.58983593e-01,  1.88693598e+00, -1.85259358e-01,
#         -1.39272935e+00,  1.08820663e-01, -9.56834437e-02,
#          1.65259423e+00,  2.14453187e+00,  4.13840915e-01,
#         -3.85444719e-01,  1.40121355e+00, -5.04570496e-01,
#         -4.38490314e-02, -2.12349972e-01, -2.92982888e-01,
#         -1.72816430e+00, -1.87039868e+00, -1.61168285e+00,
#          1.02894942e-02, -1.04761340e-01],
#        [ 1.88823170e-01,  1.12951053e+00,  1.04128459e-01,
#         -9.03233020e-01, -9.29043246e-01,  4.96563250e-01,
#          2.18149660e+00,  8.94252237e-01,  1.31061384e-01,
#         -1.45372730e-01,  1.22559265e+00, -2.11495399e-02,
#         -1.58711053e-01,  4.53622512e-02, -6.51861588e-01,
#          3.93709243e-01, -6.18468663e-01, -6.46303301e-01,
#          4.86789988e-01, -2.39651864e-01],
#        [ 6.44591541e-01,  1.61330496e-01,  1.16853911e+00,
#          1.27726333e+00,  4.07457875e-01, -3.28153142e+00,
#         -9.03008522e-01, -1.88700507e+00, -7.36675909e-01,
#          7.92009533e-01,  1.94865847e-01,  9.80165855e-01,
#         -1.87548846e-01, -1.97708037e+00,  1.65135293e+00,
#          1.65213978e+00,  2.12271501e+00,  1.71466020e+00,
#         -9.05447320e-02, -4.58806518e+00],
#        [ 1.08399753e+00,  5.86162085e-01,  2.12699514e+00,
#          1.99212747e+00,  3.40727253e-01, -3.42256756e+00,
#         -1.95479724e+00, -2.88975907e+00, -1.23850805e+00,
#          6.18729114e-01,  2.90218684e-01,  1.77424580e+00,
#         -5.79813814e-02, -1.81960469e+00,  1.39268718e+00,
#          1.74264829e+00,  2.53717518e+00,  2.57056345e+00,
#         -4.87832817e-01, -2.86517647e+00],
#        [ 2.22720691e-01,  1.07617891e+00,  1.18698543e+00,
#         -4.59485813e+00, -5.68079834e-01,  2.14370778e+00,
#          8.12130931e-01,  1.56530060e+00,  7.90326846e-01,
#          1.20231576e+00,  7.47117643e-01, -1.22794812e+00,
#          2.61104274e-01, -3.82008265e-01,  1.35735136e+00,
#         -2.38362335e+00, -4.53667023e+00, -4.93589714e+00,
#          1.28194834e-01, -7.28829266e-02],
#        [-8.81049445e-03,  1.43020040e+00, -2.08596109e-01,
#         -1.44462838e+00, -4.12927920e-01, -1.70268562e-01,
#          2.20402257e+00,  2.80599912e+00,  9.78547621e-01,
#         -3.62991359e-01,  1.68524277e+00, -8.94427127e-01,
#          5.04565692e-01, -5.52712017e-02, -5.77562530e-01,
#         -1.52451968e+00, -1.57896932e+00, -1.57799341e+00,
#          3.46750613e-01, -3.47091737e-01],
#        [ 5.41912579e-01,  1.71804497e+00, -6.16302404e-01,
#         -1.24821911e+00,  5.85669631e-02,  1.26000807e-01,
#          1.19670576e+00,  2.34247320e+00,  8.95960781e-01,
#         -2.96406721e-01,  1.75400384e+00, -7.50192410e-01,
#         -6.74999258e-02,  6.52167787e-02, -2.38934299e-01,
#         -1.22290665e+00, -1.75140412e+00, -2.03900912e+00,
#         -2.04923550e-01, -3.30885046e-01],
#        [ 1.80688015e+00,  1.46262420e+00, -4.84819094e-01,
#          1.45435849e+00, -6.75119700e+00, -5.01070547e-01,
#          4.10970787e-02, -1.15835451e+01,  1.37092253e+00,
#          3.75316643e-01,  1.62618877e+00,  8.12095855e-01,
#          8.21574329e-01,  1.04987815e+00,  2.05468397e+00,
#          1.55266237e-01,  1.34906900e+00,  1.22559653e+00,
#          1.77282274e-01, -9.53143686e-01],
#        [ 1.45638242e-01,  1.43710724e+00, -1.76989148e-01,
#         -1.24902622e+00,  6.06825083e-02, -4.88342129e-03,
#          2.22922428e+00,  2.70983790e+00,  7.95936416e-01,
#         -3.19179909e-01,  1.46855956e+00, -9.05932970e-01,
#          1.88414307e-01, -2.47775047e-03, -7.01394114e-01,
#         -1.50441722e+00, -1.93201343e+00, -1.34651053e+00,
#          8.42343194e-02, -3.82542801e-01],
#        [-4.30039637e-01, -1.02925201e+00,  8.24000780e-01,
#          1.35105825e+00,  8.50533851e-01,  1.12134240e+00,
#         -8.21987466e-01, -2.84034619e+00, -8.43312469e-01,
#          2.93770335e-01, -1.19463566e+00,  7.90435222e-01,
#         -1.00547482e-01,  5.95908515e-01, -1.03979822e-01,
#         -6.12950531e+00,  1.43776587e+00,  1.58698512e+00,
#          4.19632883e-01,  1.64837816e+00],
#        [ 2.16167281e-01, -1.05463272e+00,  9.50359160e-01,
#          9.47208813e-01,  5.23720543e-01,  1.80363174e+00,
#         -1.35120095e+00, -3.23397545e+00, -4.70209167e-01,
#          1.16006856e-01, -1.46845158e+00,  9.73754583e-01,
#          5.43652634e-01,  8.25874696e-01,  5.30055786e-03,
#         -6.55784213e+00,  1.46131410e+00,  1.62924002e+00,
#          8.29760509e-01,  1.57800360e+00],
#        [-5.14675096e-01, -1.41760358e+00,  9.97279149e-01,
#          4.10117492e-01,  9.72798020e-01,  6.66106612e-01,
#         -2.24452814e+00, -1.37813643e+00,  3.89542716e-01,
#          4.91591727e-01, -1.38517530e+00,  2.65770741e-02,
#          3.38728205e-01,  5.38498157e-01,  1.53207255e+00,
#          7.33106655e-01,  1.48423592e+00,  1.50852808e+00,
#          5.49865782e-01,  1.59735860e+00],
#        [-7.73146712e-02,  1.51948957e+00, -1.05674696e-01,
#         -1.32739309e+00,  1.25994754e-01,  4.34908812e-01,
#          1.91621718e+00,  2.42256416e+00,  1.00686905e+00,
#         -5.86648343e-01,  1.70673954e+00, -3.98291844e-01,
#          2.91102188e-01, -4.41350965e-01, -3.79084504e-01,
#         -1.60067848e+00, -1.81866129e+00, -2.08939827e+00,
#         -8.54279120e-02, -5.82266936e-01],
#        [ 2.66600789e-01, -4.71505583e-01, -4.52482901e-02,
#          9.22653462e-02, -2.99394898e-01, -2.75967529e-01,
#         -2.34555368e-01,  1.25970239e-02, -1.01721159e-01,
#         -3.04004699e-01, -4.12949010e-01,  1.48283070e-01,
#         -2.36259729e-01, -2.29048677e-01, -2.22313232e-02,
#          8.57280273e-02, -3.51127715e-01,  1.28590969e-01,
#         -9.50717945e-02, -1.35144288e-01],
#        [ 2.73736963e-01,  1.36103868e+00,  2.22374562e-01,
#         -8.19441752e-01, -6.71279611e-02,  6.04209038e-02,
#          2.29909705e+00,  2.42761015e+00,  1.06909467e+00,
#         -2.66622698e-01,  1.44985520e+00, -7.96688842e-01,
#          5.35287306e-01, -2.58277844e-01, -3.47720565e-01,
#         -1.23883379e+00, -2.01391473e+00, -1.80525901e+00,
#          2.01015553e-02, -6.69038729e-01],
#        [ 4.06720339e-01,  1.19438842e+00, -1.67514005e-02,
#         -9.87119345e-01, -2.29602510e-01,  2.79023708e-01,
#          2.12578711e+00,  2.37076114e+00,  7.33555506e-01,
#         -6.07842219e-02,  1.19067871e+00, -7.91466407e-01,
#          1.85671285e-01, -1.65009712e-01, -7.18838833e-01,
#         -1.54390861e+00, -1.88828487e+00, -1.84499477e+00,
#         -3.92167951e-02, -6.31078959e-01],
#        [ 3.88040674e-01,  1.30161194e+00,  1.53453146e-01,
#         -1.23817866e+00,  1.57384747e-01,  2.84638529e-01,
#          1.98261752e+00,  2.42343926e+00,  9.39968785e-01,
#         -3.92636977e-01,  1.63512244e+00, -8.85872443e-01,
#          3.26419097e-01,  3.19320432e-02, -5.67924815e-01,
#         -1.66610557e+00, -1.67319295e+00, -2.07654328e+00,
#         -7.73351659e-02, -6.27130330e-01]]), array([[ 3.02430977, -0.27775925,  3.66186378, -2.4141096 , -0.95847418,
#          0.87703999,  1.11663674, -4.35685275, -0.40762394, -3.17052496,
#         -2.31777001, -0.13333401,  1.36961848,  0.90893292,  2.61821151,
#         -1.26315742, -0.27126498, -1.38980792, -0.56339561, -1.70401449],
#        [-3.24029403, -0.44675693, -3.81741206,  3.20180746,  1.17428528,
#         -1.07168372, -1.178447  ,  4.22488094,  0.58926309,  2.80903929,
#          2.44771465,  0.61630762, -1.20496997, -0.64215315, -2.51415286,
#          1.69077211, -0.14875603,  1.58851701, -0.02171227,  1.74563484]])]

print(net_softmax.get_all_biases())
# [array([ 0.60935182,  0.81446946,  0.2858068 ,  0.82678005,  1.40549814,
#         0.44416512,  0.07158877,  0.26439437,  1.10452586,  1.23764422,
#         0.8886213 ,  1.53782536,  0.69559686,  0.66763001,  1.30899841,
#        -0.70313611,  0.31809903,  0.06720147,  0.63249135,  0.50187307]), array([0.11237886, 0.07821266, 0.47378935, 1.39613725, 0.90527368,
#        0.67463353, 0.15618653, 0.49931045, 1.11579554, 0.9105537 ,
#        0.45906837, 1.31565317, 0.54146272, 0.70262141, 0.07340024,
#        0.56424612, 0.10147306, 0.48171974, 1.17121183, 0.99475092]), array([1.02714937, 0.22658331])]