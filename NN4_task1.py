#%% 

import numpy as np
from pandas import read_csv
import matplotlib.pyplot as plt
from model import Net
import math
import pickle
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import f1_score

#%%

def save(array, file_name):
    file = open(file_name, 'wb')
    pickle.dump(array, file)
    file.close()

def read(filename):
    with open(filename, 'rb') as file:
        array = pickle.load(file)
    return array

def ReLU(x):
    if x > 0:
        return x
    return 0.0

def sigma(x):
    if x > 0:
        return 1 / (1 + math.e ** ((-1) * x))
    return math.e ** x / (1 + math.e ** x)

def predict_class(predictions):
    classes = []
    for p in predictions:
        classes.append(np.where(np.max(p) == p))
    return np.array(classes).reshape(-1)

def predict(net, x_data):
    predictions = []
    for x in x_data:
        predictions.append(net.predict(x))
    return np.array(predictions)

#%%

df_train = read_csv("data/classification/rings3-regular-training.csv")
df_train.head()

xy_train = df_train[["x", "y"]].to_numpy()
c_train = df_train["c"].to_numpy()

df_test = read_csv("data/classification/rings3-regular-test.csv")
df_test.head()

xy_test = df_test[["x", "y"]].to_numpy()
c_test= df_test["c"].to_numpy()

#%%
        
plt.scatter(xy_train[:,0], xy_train[:,1], c=(c_train+0.5)/2)
plt.show()

#%% scaling

scaler_xy = MinMaxScaler(feature_range=(-0.9, 0.9))
xy_train_scaled = scaler_xy.fit_transform(xy_train)
xy_test_scaled = scaler_xy.transform(xy_test)

#%%

f = [sigma, sigma, "softmax"]
net_softmax = Net(n_neurons=[20, 20, 3], n_inputs=2, functions=f, param_init='xavier', use_softmax=True)
f = [sigma, sigma, lambda x: x]
net_basic = Net(n_neurons=[20, 20, 3], n_inputs=2, functions=f, param_init='xavier', classification=True)

#%%

epoch = 1
score_softmax = 0
score_basic = 0
scores_softmax = []
scores_basic = []
epochs = []
while score_softmax < 0.75:
    epochs.append(epoch)
    epoch += 1
    net_softmax.fit(xy_train_scaled, c_train, batch_size=1, epochs=1, alpha=0.003,
                      method='rmsprop', m_lambda=0.9)
    net_basic.fit(xy_train_scaled, c_train, batch_size=1, epochs=1, alpha=0.003,
                      method='rmsprop', m_lambda=0.9)
    preds = predict(net_softmax, xy_test_scaled)
    classes = predict_class(preds)
    score_softmax = f1_score(c_test, classes, average='weighted')
    scores_softmax.append(score_softmax)
    preds = predict(net_basic, xy_test_scaled)
    classes = predict_class(preds)
    score_basic = f1_score(c_test, classes, average='weighted')
    scores_basic.append(score_basic)
    print(epoch - 1)
    print('s: ', score_softmax)
    print('b: ', score_basic)
    print()
    
#%%

plt.plot(epochs, scores_softmax, 'o-')
plt.plot(epochs, scores_basic, 'o-')
plt.legend(('softmax', 'basic'), loc='upper left')
plt.xlabel('epoch')
plt.ylabel('F1 score')
plt.show()

print('Current epoch: ', epoch - 1)
print('F1 score softmax: ', score_softmax)
print('F1 score basic: ', score_basic)

#%%

print(net_softmax.get_all_weights())
# [array([[-3.4971414 , -0.05634948],
#        [-5.60116104, -5.06872705],
#        [-1.85808127, -0.08616066],
#        [-0.11615348, -1.80450189],
#        [ 1.13588117,  0.79901355],
#        [-1.9122249 , -0.23942784],
#        [-2.87343842,  0.3757048 ],
#        [-4.2763683 ,  0.01962045],
#        [ 1.63033227,  0.31024537],
#        [-3.77674454,  0.28818716],
#        [-3.80618228,  0.2226087 ],
#        [ 3.42231307, -0.50757572],
#        [-3.37124123, -0.24973073],
#        [ 2.91545286, -0.26219547],
#        [ 0.31397939,  5.33118536],
#        [-2.46757147, -0.05680213],
#        [ 2.18186215,  0.11427798],
#        [ 1.85631733,  0.54279837],
#        [ 1.24043042,  0.47142994],
#        [ 2.40698071, -0.27132011]]), array([[ 2.27229564e-01,  7.60627478e-01,  8.51211719e-01,
#          2.31500142e-01,  5.92220835e-01,  6.71434688e-01,
#          1.29191066e+00,  9.76460276e-01,  8.45866279e-01,
#          4.96865536e-01,  6.62248330e-01,  2.98710041e-01,
#          2.21984373e-01,  8.03271860e-01,  3.38274486e-01,
#          5.20364803e-01,  4.86430293e-01,  1.13768362e+00,
#          1.05519852e+00,  4.83467950e-01],
#        [-1.23108396e+00, -4.72193875e+00, -1.80052438e+00,
#         -1.87155331e+00, -6.27969631e-01, -1.69029761e+00,
#         -1.14328919e+00, -1.90370049e+00, -2.22409472e-01,
#         -1.06423535e+00, -1.26494535e+00,  1.45483504e+00,
#         -1.94537633e+00,  7.20515324e-01, -4.70810352e-01,
#         -2.03729453e+00,  6.95791414e-01, -1.19454994e-02,
#         -8.27676917e-01,  9.77581839e-01],
#        [-9.59211980e-01,  2.18999179e+00, -4.01710228e-01,
#          1.88175483e-01,  7.23756462e-01, -8.21417292e-01,
#         -5.82269720e-01, -9.55063549e-01,  1.37885779e+00,
#         -5.77227121e-01, -5.14183132e-01,  3.19863771e+00,
#         -9.26482180e-01,  2.80162076e+00,  9.06571239e-01,
#         -8.93086271e-01,  1.72934330e+00,  9.37758028e-01,
#          5.56479190e-01,  1.77625080e+00],
#        [-7.38129694e-02, -9.93086599e-03,  4.45095630e-01,
#         -1.19619672e+00,  1.19811414e-02,  5.70574426e-01,
#          7.54774266e-01,  6.88202687e-02,  1.04214615e-01,
#          3.60155228e-01,  1.96856390e-01, -5.61162897e-01,
#         -1.60466439e-01, -2.71995765e-01, -1.16191757e+00,
#          4.52637410e-01, -1.96454226e-01, -4.24360190e-01,
#          5.79179803e-01, -1.35738806e-01],
#        [-2.33218189e+00, -2.55445337e+00, -1.07039257e+00,
#         -1.69922675e+00,  4.27332114e-01, -1.62246133e+00,
#         -1.39813130e+00, -2.47151371e+00,  6.91300035e-01,
#         -2.05272712e+00, -2.44381585e+00,  8.53153480e-01,
#         -1.96824564e+00,  1.00784520e+00, -9.44512085e-01,
#         -2.14868064e+00,  6.39197209e-01,  6.70605245e-01,
#          4.29122764e-01,  8.77006160e-01],
#        [-1.78068045e+00, -9.98614226e-01, -9.18164866e-01,
#          1.00397404e-01,  3.25293523e-01, -7.51233466e-01,
#         -1.51574863e+00, -2.41968342e+00,  1.37008951e+00,
#         -2.26549431e+00, -2.32674865e+00,  2.17079833e+00,
#         -1.24301364e+00,  1.70575604e+00,  2.91539960e-01,
#         -1.77141683e+00,  1.12638805e+00,  5.30323767e-01,
#          6.35157465e-01,  1.85948049e+00],
#        [-2.06196481e+00, -1.50772066e+00, -1.25872250e+00,
#         -1.63209134e+00,  5.94229001e-01, -1.23526679e+00,
#         -1.69805848e+00, -2.61308201e+00,  1.04459291e+00,
#         -1.95622549e+00, -2.00697622e+00,  1.06955975e+00,
#         -2.08936763e+00,  1.31603232e+00, -1.54502798e+00,
#         -1.99799950e+00,  6.10967607e-01,  7.19176976e-01,
#          4.74861894e-01,  9.72271287e-01],
#        [ 6.28784901e-01, -1.70185273e+00,  8.61835701e-01,
#          6.42410111e-01, -1.12320990e+00,  7.76112426e-01,
#          5.35652589e-01, -1.87799729e-01, -1.13602908e+00,
#          5.63302364e-01,  4.96239983e-01, -1.75271637e+00,
#          1.53622561e-01, -1.76383534e+00, -1.52516698e+00,
#          6.32586973e-01, -1.74169184e+00, -1.32366976e+00,
#         -6.31280755e-01, -1.59335835e+00],
#        [-3.36165297e+00, -2.23431506e+00, -1.75971945e+00,
#         -2.35180140e+00, -6.27240015e-01, -2.23694738e+00,
#         -2.24303995e+00, -3.37204189e+00, -5.86193023e-01,
#         -3.29362683e+00, -3.28988029e+00, -4.63342257e-01,
#         -2.97035290e+00, -1.00558152e+00, -1.25940081e+00,
#         -2.15452712e+00, -5.63531949e-01, -8.43125671e-01,
#         -2.49391098e-01, -9.03097175e-01],
#        [ 4.98063603e-01, -2.86400986e+00,  7.51157426e-01,
#          3.61400331e-01, -1.13359466e+00,  5.79916337e-01,
#          9.66816497e-01,  3.39706257e-01, -1.67471845e+00,
#          5.90912980e-01,  8.22354033e-01, -2.45171096e+00,
#          6.14816953e-01, -2.38540626e+00, -1.09227494e+00,
#          3.39020191e-01, -1.85905214e+00, -1.05404687e+00,
#         -1.10960971e+00, -2.26396334e+00],
#        [-6.32404239e-01,  2.00369731e+00, -3.59584988e-01,
#         -2.21646067e-01,  2.35909762e-01, -7.70379208e-01,
#         -5.06346465e-01, -7.00362850e-01,  1.57267675e+00,
#         -6.83348729e-01, -1.07217165e+00,  3.48738509e+00,
#         -2.34885604e-01,  2.99938540e+00,  5.33211142e-01,
#         -9.63690162e-01,  1.97456488e+00,  9.59081585e-01,
#          8.57388731e-01,  2.01519933e+00],
#        [-6.00392954e-01, -5.26294413e+00,  2.28776303e-01,
#         -1.44387479e-01,  4.24141486e-01,  1.75040085e-02,
#         -3.02888986e-01, -1.69827438e+00,  1.24541739e+00,
#         -6.26648001e-01, -8.94242431e-01,  4.91972466e-01,
#         -6.19698153e-01,  2.34638669e-01,  1.18706140e+00,
#         -6.26496365e-01,  1.40618507e+00,  4.46513060e-01,
#          7.56441576e-01,  6.31942417e-01],
#        [-3.16914763e+00, -1.53355100e+00, -1.83100627e+00,
#         -1.73496731e+00,  6.19521412e-02, -1.32295930e+00,
#         -2.30546058e+00, -4.13567352e+00,  3.21797291e-01,
#         -2.93476747e+00, -3.47234988e+00,  2.33044650e-02,
#         -2.37843510e+00, -3.45849258e-01, -8.80086674e-01,
#         -1.96853732e+00, -1.51657065e-02,  3.20551505e-02,
#          2.31887291e-01,  4.55413514e-03],
#        [-1.32237104e+00, -2.15922810e+00, -7.73559055e-01,
#         -8.78162557e-02, -5.67685328e-01, -5.87792451e-01,
#         -7.02430599e-01, -1.78798134e+00, -3.77578000e-01,
#         -8.98646364e-01, -1.84024401e+00,  1.12328483e+00,
#         -1.56444952e+00,  9.78978640e-01,  7.00919213e-01,
#         -1.12008885e+00, -2.03072620e-01, -6.56342948e-01,
#         -2.54085843e-01,  1.66128301e-01],
#        [-1.02707805e+00, -1.44933829e+00, -7.07380399e-01,
#         -6.95878757e-01, -3.06982329e-01, -3.46050011e-01,
#         -4.04321502e-01, -8.17226336e-01, -2.04148584e-01,
#         -6.51463895e-01, -1.28247289e+00,  8.62523346e-01,
#         -9.37294609e-01,  2.60134834e-01,  1.20980930e+00,
#         -5.59146324e-01,  4.47117480e-01,  8.73905932e-02,
#         -2.19832634e-01,  5.90344855e-01],
#        [-2.56540456e+00,  3.45760523e+00, -1.64750310e+00,
#         -1.70557201e+00,  3.23872659e-01, -1.63744573e+00,
#         -1.69771058e+00, -3.13667694e+00,  3.11596679e-01,
#         -2.33509762e+00, -2.67211739e+00,  7.70286776e-01,
#         -2.83862857e+00,  7.34493142e-01, -2.06367882e+00,
#         -2.73121496e+00,  5.75920537e-01, -1.26357004e-01,
#          4.66634589e-01,  9.28309456e-01],
#        [-6.44165556e-01, -3.18675021e+00, -7.90959557e-02,
#         -7.41507997e-01,  2.65946046e-01,  1.37683094e-03,
#          3.46646719e-01, -9.67922914e-01,  8.11343174e-01,
#         -3.26525494e-01, -2.58878835e-01,  1.87532667e+00,
#         -1.48432627e-01,  1.73517047e+00,  6.78423546e-01,
#         -5.92232911e-01,  1.33517229e+00,  6.24035811e-01,
#          4.08977584e-01,  1.35292582e+00],
#        [-6.51531191e-01,  3.49169553e+00, -1.04030409e+00,
#         -5.09629956e-03, -9.68622717e-02, -9.30452681e-01,
#         -1.05392910e+00, -4.59593655e-01,  1.05133036e+00,
#         -1.13587881e+00, -7.13375572e-01,  2.84432946e+00,
#         -6.94473963e-01,  1.99459329e+00,  8.68507868e-02,
#         -1.25092721e+00,  1.29535990e+00,  8.35311927e-01,
#          7.09237660e-01,  2.10320783e+00],
#        [-4.82998507e+00, -1.47962485e+00, -2.69461056e+00,
#         -2.09375255e+00,  6.12578579e-01, -2.39433091e+00,
#         -3.11927104e+00, -5.62777485e+00,  8.00184094e-01,
#         -4.54497794e+00, -4.50061150e+00,  9.67540411e-01,
#         -4.12015382e+00,  2.97329550e-01, -1.95758131e+00,
#         -3.26139466e+00,  3.48881187e-01,  2.17638173e-01,
#          1.12992265e-01,  7.64783490e-01],
#        [ 7.54213355e-01, -1.82809394e+00,  3.35986597e-03,
#          3.43056666e-01, -1.25686210e+00,  2.65036469e-01,
#          2.98647725e-01,  7.09329858e-01, -2.19567321e+00,
#          9.99851351e-01,  6.79422662e-01, -1.26345625e+00,
#          6.26944365e-01, -9.50727655e-01, -2.08604647e+00,
#          1.05363663e+00, -1.76331761e+00, -1.26034659e+00,
#         -1.81045558e+00, -1.32611778e+00]]), array([[ -1.23710062,  -1.59537668,  -2.3344795 ,  -1.28421928,
#          -5.32926993,   0.9030327 ,  -5.82035996,   1.86644932,
#          -2.31048873,   3.04369322,  -2.63035839,   0.98277304,
#          -3.62456898,  -1.43994639,  -1.79444375,  -7.39393042,
#          -1.135755  ,  -1.89305065, -11.63389269,   0.48139068],
#        [ -1.75111377,  -0.88548344,  -0.79388227,  -1.37215529,
#          -1.06662945,  -2.73269956,  -1.06515012,  -7.46367369,
#          -2.15693018, -11.93005195,  -1.1040891 ,  -3.25332198,
#          -2.39052787,  -2.01447044,  -2.12635778,  -2.60192294,
#          -0.98202173,  -0.71634444,  -3.56853825,  -7.26986522],
#        [ -0.21930992,  -1.61520753,  -1.28298058,  -0.12153605,
#          -1.37820731,  -4.44498999,  -1.30309753,  -2.6629941 ,
#          -0.19882273,  -3.04197793,  -1.66874099,  -1.18542401,
#           0.50867665,  -3.33347755,  -3.5015749 ,   1.55492247,
#          -3.58729132,  -1.5670527 ,   1.4680127 ,  -2.90028125]])]

print(net_softmax.get_all_biases())
# [array([ 0.09275802, -0.79894089, -1.08188703, -0.84422678, -0.21961319,
#        -1.07287633, -0.95266778,  0.6310073 , -0.71671938, -0.45197654,
#         0.15522467,  1.15750715,  0.51587408,  1.08861005,  2.69978639,
#        -0.77294732, -0.01172333,  0.67665712, -0.82910094,  0.49659522]), array([ 1.8864003 , -0.97948801,  0.24071147,  0.88817857,  0.4670635 ,
#         0.69477879,  0.58474425,  0.62623685, -0.39334467,  0.62097187,
#        -0.05272547,  0.75384728,  0.51969196,  0.1555859 , -0.0287084 ,
#         0.24584664,  0.79475244, -0.05802247,  0.62785071,  0.30810419]), array([-1.76484237, -1.21169656, -0.04936761])]